{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Toxicity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset : https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "This contains comments in textual form that are classified on the level of toxicity and it's nature.\n",
    "We will build an LSTM based model to predict the toxicity of new comments.\n",
    "\n",
    "The data is multi - output meaning that each record may correspond to more than one output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571,)\n",
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text for processing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "X = df['comment_text']  # comment text\n",
    "Y = df[df.columns[2:]]; # drop everything other than labels\n",
    "\n",
    "# X.head()\n",
    "# Y.head()\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.values # converting the labels to an numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to vectorize the text, each word maps to a unique id. This will be done by TextVectorization function\n",
    "# So we need to create a vocabulary, set the maximum number of words allowed in the vocabulary\n",
    "# help(TextVectorization)\n",
    "\n",
    "# Some Info on TextVectorization\n",
    "\n",
    "# The vocabulary for the layer must be either supplied on construction or\n",
    "#  |  learned via `adapt()`. When this layer is adapted, it will analyze the\n",
    "#  |  dataset, determine the frequency of individual string values, and create a\n",
    "#  |  vocabulary from them. This vocabulary can have unlimited size or be capped,\n",
    "#  |  depending on the configuration options for this layer; if there are more\n",
    "#  |  unique values in the input than the maximum vocabulary size, the most frequent\n",
    "#  |  terms will be used to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000 # Vocbulary size\n",
    "# Create the vectorizer\n",
    "# help(TextVectorization)\n",
    "vectorizer = TextVectorization(max_tokens = MAX_FEATURES,output_sequence_length = 1800, output_mode = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)\n",
    "type(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer.adapt(X.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([286 261   0 ...   0   0   0], shape=(1800,), dtype=int64)\n",
      "tf.Tensor([  286    28   109     9 24198], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer.get_vocabulary()\n",
    "print(vectorizer(\"Hello world\"))\n",
    "\n",
    "print(vectorizer(\"Hello my name is Rahul\")[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize all the comments\n",
    "vectorized_text = vectorizer(X.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
       "array([[  643,    76,     2, ...,     0,     0,     0],\n",
       "       [    1,    54,  2506, ...,     0,     0,     0],\n",
       "       [  425,   440,    70, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [32141,  7329,   383, ...,     0,     0,     0],\n",
       "       [    5,    12,   533, ...,     0,     0,     0],\n",
       "       [    5,     8,   130, ...,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text\n",
    "# Each comment is allowed a maximum of 1800 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a tensorflow data pipe line\n",
    "# map, cache, shuffle, batch, prefetch\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,Y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)   # Buffer-size\n",
    "dataset = dataset.batch(16)         # Batch size\n",
    "dataset = dataset.prefetch(16)      # Pre-fetching helps prevent bottlenecks\n",
    "# dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current \n",
    "# element is being processed. This often improves latency and throughput, at the cost of using \n",
    "# additional memory to store prefetched elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchX, batchY = dataset.as_numpy_iterator().next()\n",
    "len(batchX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 70% of the dataset is taken for test data\n",
    "# then 20% is taken for validataion\n",
    "# 10% for testing\n",
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6981\n"
     ]
    }
   ],
   "source": [
    "print(len(train)) # This number is actually the number of batches of training data\n",
    "# Each batch contains 16 comments\n",
    "\n",
    "train_generator = train.as_numpy_iterator()\n",
    "# train_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# help(Embedding)\n",
    "# Embedding Layer\n",
    "model.add(Embedding(MAX_FEATURES+1,32))\n",
    "# Biderctional LSTM\n",
    "model.add(Bidirectional(LSTM(32,activation = 'tanh')))\n",
    "# Fully Connected layer\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer \n",
    "model.add(Dense(6, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          6400032   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               16640     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,442,278\n",
      "Trainable params: 6,442,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6981/6981 [==============================] - 5290s 757ms/step - loss: 0.0643 - val_loss: 0.0464\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(train, epochs=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Comment_toxicity.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the model perfomance and make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.06431057304143906], 'val_loss': [0.04635939747095108]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Comment is :  toxic obscene insult "
     ]
    }
   ],
   "source": [
    "# Let us test the model by using our own string \n",
    "# Tokenise the input string and use predict() to get the output\n",
    "# Then use an threshold of 0.5 for detecting the labels\n",
    "Text = \"You freaking suck! I am going to hit you.\" \n",
    "\n",
    "input_text = vectorizer(Text)\n",
    "\n",
    "res = model.predict(np.expand_dims(input_text,0))\n",
    "# print(res)\n",
    "# print((res > 0.5).astype(int))\n",
    "labels = list(df.columns[2:])\n",
    "output = (res > 0.5).astype(int)\n",
    "print(\"The Comment is : \",end = ' ')\n",
    "for i,x in enumerate(output[0]):\n",
    "    if(x==1):\n",
    "        print(labels[i],end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_y = test.as_numpy_iterator().next()\n",
    "# (model.predict(batch_X) > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "Pre = Precision()\n",
    "Re = Recall()\n",
    "Acc = CategoricalAccuracy()\n",
    "\n",
    "for batch in test.as_numpy_iterator(): \n",
    "    # Unpack the batch \n",
    "    X_true, y_true = batch\n",
    "    # Make a prediction \n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    Pre.update_state(y_true, yhat)\n",
    "    Re.update_state(y_true, yhat)\n",
    "    Acc.update_state(y_true, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8321626782417297, Recall:0.6549316048622131, Accuracy:0.47843530774116516\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {Pre.result().numpy()}, Recall:{Re.result().numpy()}, Accuracy:{Acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading gradio-3.0.19-py3-none-any.whl (5.1 MB)\n",
      "     ---------------------------------------- 5.1/5.1 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.0.3)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.6/53.6 KB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (3.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.14.1-cp35-abi3-win_amd64.whl (1.8 MB)\n",
      "     ---------------------------------------- 1.8/1.8 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting analytics-python\n",
      "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (2.26.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.6/140.6 KB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (8.4.0)\n",
      "Collecting markdown-it-py[linkify,plugins]\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.5/84.5 KB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (1.21.5)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.6/54.6 KB ? eta 0:00:00\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.7.2-cp39-none-win_amd64.whl (189 kB)\n",
      "     -------------------------------------- 189.5/189.5 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting paramiko\n",
      "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
      "     -------------------------------------- 212.9/212.9 KB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydub in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Collecting python-multipart\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-win_amd64.whl (554 kB)\n",
      "     -------------------------------------- 554.9/554.9 KB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2) (2.0.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.1/122.1 KB 3.6 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-win_amd64.whl (33 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->gradio) (2.0.9)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->gradio) (21.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from analytics-python->gradio) (1.16.0)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from analytics-python->gradio) (2.8.2)\n",
      "Collecting backoff==1.10.0\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gradio) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gradio) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->gradio) (3.3)\n",
      "Collecting starlette==0.19.1\n",
      "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.3/63.3 KB 3.3 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.9.1-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
      "     ---------------------------------------- 80.6/80.6 KB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from starlette==0.19.1->fastapi->gradio) (4.0.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 43.7/43.7 KB 1.0 MB/s eta 0:00:00\n",
      "Collecting linkify-it-py~=1.0\n",
      "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->gradio) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->gradio) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->gradio) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->gradio) (4.28.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->gradio) (2021.3)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.2-cp36-abi3-win_amd64.whl (29 kB)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-win_amd64.whl (212 kB)\n",
      "     -------------------------------------- 212.1/212.1 KB 2.6 MB/s eta 0:00:00\n",
      "Collecting cryptography>=2.5\n",
      "  Downloading cryptography-37.0.2-cp36-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from uvicorn->gradio) (8.0.3)\n",
      "Collecting asgiref>=3.4.0\n",
      "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.2/58.2 KB ? eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.1 in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click>=7.0->uvicorn->gradio) (0.4.4)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rahul raaghav a\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
      "Building wheels for collected packages: ffmpy, python-multipart\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=516d7dd9a8f177b41337e263a47a7c40315e66964d736eb0306f90371bcfe6f5\n",
      "  Stored in directory: c:\\users\\rahul raaghav a\\appdata\\local\\pip\\cache\\wheels\\91\\e2\\96\\f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
      "  Building wheel for python-multipart (setup.py): started\n",
      "  Building wheel for python-multipart (setup.py): finished with status 'done'\n",
      "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31671 sha256=bedb1eccf2eb046d2255c2317b5b4067cb9fc453ec1a23bbd8811e4f11565b12\n",
      "  Stored in directory: c:\\users\\rahul raaghav a\\appdata\\local\\pip\\cache\\wheels\\fe\\04\\d1\\a10661cc45f03c3cecda50deb2d2c22f57b4e84a75b2a5987e\n",
      "Successfully built ffmpy python-multipart\n",
      "Installing collected packages: monotonic, ffmpy, uc-micro-py, sniffio, python-multipart, pydantic, pycryptodome, orjson, multidict, mdurl, h11, fsspec, frozenlist, backoff, async-timeout, asgiref, yarl, uvicorn, pynacl, markdown-it-py, linkify-it-py, cryptography, bcrypt, anyio, analytics-python, aiosignal, starlette, paramiko, mdit-py-plugins, aiohttp, fastapi, gradio\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.6.1 asgiref-3.5.2 async-timeout-4.0.2 backoff-1.10.0 bcrypt-3.2.2 cryptography-37.0.2 fastapi-0.78.0 ffmpy-0.3.0 frozenlist-1.3.0 fsspec-2022.5.0 gradio-3.0.19 h11-0.13.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 multidict-6.0.2 orjson-3.7.2 paramiko-2.11.0 pycryptodome-3.14.1 pydantic-1.9.1 pynacl-1.5.0 python-multipart-0.0.5 sniffio-1.2.0 starlette-0.19.1 uc-micro-py-1.0.1 uvicorn-0.17.6 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Comment_toxicity.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_comment(comment):\n",
    "    vectorized_comment = vectorizer([comment])\n",
    "    results = model.predict(vectorized_comment)\n",
    "    \n",
    "    text = ''\n",
    "    for idx, col in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(col, results[0][idx]>0.5)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul raaghav A\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\Rahul raaghav A\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "c:\\Users\\Rahul raaghav A\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    }
   ],
   "source": [
    "interface = gr.Interface(fn=score_comment, \n",
    "                         inputs=gr.inputs.Textbox(lines=2, placeholder='Comment to score'),\n",
    "                        outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860/\n",
      "Running on public URL: https://45578.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://45578.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x19e3fe99490>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://45578.gradio.app')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e2d4bb3fb4a74752fb0ab9ee189e7ae19a681ce247d36d7b7485e6058434b21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
